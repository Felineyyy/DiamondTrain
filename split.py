import json
import random
import argparse
import os
from collections import defaultdict

def split_dataset(json_file, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, output_dir="./"):
    """
    åˆ†å‰²æ•°æ®é›†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†
    
    Args:
        json_file: åŸå§‹COCOæ ¼å¼æ ‡æ³¨æ–‡ä»¶
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹ (é»˜è®¤0.7)
        val_ratio: éªŒè¯é›†æ¯”ä¾‹ (é»˜è®¤0.2) 
        test_ratio: æµ‹è¯•é›†æ¯”ä¾‹ (é»˜è®¤0.1)
        output_dir: è¾“å‡ºç›®å½•
    """

    print("å¼€å§‹åˆ†å‰²æ•°æ®é›†...")
    print(f"è®­ç»ƒé›†: {train_ratio*100:.1f}%")
    print(f"éªŒè¯é›†: {val_ratio*100:.1f}%") 
    print(f"æµ‹è¯•é›†: {test_ratio*100:.1f}%")
    
    # è¯»å–åŸå§‹æ•°æ®
    with open(json_file, 'r') as f:
        data = json.load(f)
    
    images = data['images']
    annotations = data['annotations']
    categories = data['categories']
    
    print(f"åŸå§‹æ•°æ®é›†: {len(images)} å¼ å›¾ç‰‡, {len(annotations)} ä¸ªæ ‡æ³¨")
    
    # æŒ‰åœºæ™¯åˆ†ç»„ï¼Œç¡®ä¿æ¯ä¸ªåœºæ™¯çš„å›¾ç‰‡éƒ½èƒ½åˆ†å¸ƒåˆ°å„ä¸ªé›†åˆä¸­
    scene_groups = defaultdict(list)
    for img in images:
        # ä»æ–‡ä»¶è·¯å¾„æå–åœºæ™¯ä¿¡æ¯
        path_parts = img['file_name'].split('/')
        if len(path_parts) >= 3:
            scene = path_parts[2]  # ./dataset/water/20210603/xxx.jpg -> water
            date = path_parts[3] if len(path_parts) >= 4 else "default"
            scene_key = f"{scene}_{date}"
        else:
            scene_key = "default"
        
        scene_groups[scene_key].append(img)
    
    print(f"å‘ç° {len(scene_groups)} ä¸ªåœºæ™¯ç»„:")
    for scene, imgs in scene_groups.items():
        print(f"  {scene}: {len(imgs)} å¼ å›¾ç‰‡")
    
    # ä¸ºæ¯ä¸ªåœºæ™¯åˆ†é…å›¾ç‰‡åˆ°ä¸åŒé›†åˆ
    train_images = []
    val_images = []
    test_images = []
    
    for scene, imgs in scene_groups.items():
        # éšæœºæ‰“ä¹±è¯¥åœºæ™¯çš„å›¾ç‰‡
        random.shuffle(imgs)
        
        n_total = len(imgs)
        n_train = int(n_total * train_ratio)
        n_val = int(n_total * val_ratio)
        
        # åˆ†é…å›¾ç‰‡
        scene_train = imgs[:n_train]
        scene_val = imgs[n_train:n_train + n_val]
        scene_test = imgs[n_train + n_val:]
        
        train_images.extend(scene_train)
        val_images.extend(scene_val)
        test_images.extend(scene_test)
        
        print(f"  {scene} -> è®­ç»ƒ:{len(scene_train)}, éªŒè¯:{len(scene_val)}, æµ‹è¯•:{len(scene_test)}")
    
    # è·å–å¯¹åº”çš„æ ‡æ³¨
    def get_annotations_for_images(image_list):
        image_ids = set(img['id'] for img in image_list)
        return [ann for ann in annotations if ann['image_id'] in image_ids]
    
    train_annotations = get_annotations_for_images(train_images)
    val_annotations = get_annotations_for_images(val_images)
    test_annotations = get_annotations_for_images(test_images)
    
    # åˆ›å»ºæ–°çš„æ•°æ®é›†
    def create_dataset(images, annotations, categories):
        return {
            'images': images,
            'annotations': annotations,
            'categories': categories
        }
    
    train_data = create_dataset(train_images, train_annotations, categories)
    val_data = create_dataset(val_images, val_annotations, categories)
    test_data = create_dataset(test_images, test_annotations, categories)
    
    # ä¿å­˜åˆ†å‰²åçš„æ•°æ®é›†
    train_file = os.path.join(output_dir, "train.json")
    val_file = os.path.join(output_dir, "val.json")
    test_file = os.path.join(output_dir, "test.json")
    
    with open(train_file, 'w') as f:
        json.dump(train_data, f, indent=2)
    
    with open(val_file, 'w') as f:
        json.dump(val_data, f, indent=2)
    
    with open(test_file, 'w') as f:
        json.dump(test_data, f, indent=2)
    
    # æ‰“å°åˆ†å‰²ç»“æœ
    print(f"\nâœ… æ•°æ®é›†åˆ†å‰²å®Œæˆ!")
    print(f"è®­ç»ƒé›†: {len(train_images)} å›¾ç‰‡, {len(train_annotations)} æ ‡æ³¨ -> {train_file}")
    print(f"éªŒè¯é›†: {len(val_images)} å›¾ç‰‡, {len(val_annotations)} æ ‡æ³¨ -> {val_file}")
    print(f"æµ‹è¯•é›†: {len(test_images)} å›¾ç‰‡, {len(test_annotations)} æ ‡æ³¨ -> {test_file}")
    
    # éªŒè¯æ²¡æœ‰é‡å¤
    all_image_ids = []
    all_image_ids.extend([img['id'] for img in train_images])
    all_image_ids.extend([img['id'] for img in val_images])
    all_image_ids.extend([img['id'] for img in test_images])
    
    if len(all_image_ids) == len(set(all_image_ids)):
        print("âœ… éªŒè¯é€šè¿‡: æ²¡æœ‰é‡å¤å›¾ç‰‡")
    else:
        print("âŒ è­¦å‘Š: å‘ç°é‡å¤å›¾ç‰‡!")
    
    return train_file, val_file, test_file

def analyze_split_quality(train_file, val_file, test_file):
    """åˆ†æåˆ†å‰²è´¨é‡"""
    print("\nğŸ“Š åˆ†å‰²è´¨é‡åˆ†æ:")
    
    files = [("è®­ç»ƒé›†", train_file), ("éªŒè¯é›†", val_file), ("æµ‹è¯•é›†", test_file)]
    
    for name, file_path in files:
        with open(file_path, 'r') as f:
            data = json.load(f)
        
        # ç»Ÿè®¡åœºæ™¯åˆ†å¸ƒ
        scene_count = defaultdict(int)
        for img in data['images']:
            path_parts = img['file_name'].split('/')
            if len(path_parts) >= 3:
                scene = path_parts[2]
                scene_count[scene] += 1
        
        print(f"\n{name}:")
        print(f"  æ€»å›¾ç‰‡: {len(data['images'])}")
        print(f"  æ€»æ ‡æ³¨: {len(data['annotations'])}")
        print(f"  åœºæ™¯åˆ†å¸ƒ: {dict(scene_count)}")
        
        # è®¡ç®—å¹³å‡æ¯å¼ å›¾çš„æ ‡æ³¨æ•°
        if len(data['images']) > 0:
            avg_annotations = len(data['annotations']) / len(data['images'])
            print(f"  å¹³å‡æ ‡æ³¨/å›¾ç‰‡: {avg_annotations:.2f}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='åˆ†å‰²COCOæ•°æ®é›†')
    parser.add_argument('--input', type=str, default='trainval.json', help='è¾“å…¥çš„æ ‡æ³¨æ–‡ä»¶')
    parser.add_argument('--output-dir', type=str, default='./', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--train-ratio', type=float, default=0.7, help='è®­ç»ƒé›†æ¯”ä¾‹')
    parser.add_argument('--val-ratio', type=float, default=0.2, help='éªŒè¯é›†æ¯”ä¾‹') 
    parser.add_argument('--test-ratio', type=float, default=0.1, help='æµ‹è¯•é›†æ¯”ä¾‹')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    random.seed(args.seed)
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.input):
        print(f"âŒ æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶: {args.input}")
        exit(1)
    
    # åˆ†å‰²æ•°æ®é›†
    train_file, val_file, test_file = split_dataset(
        args.input,
        args.train_ratio,
        args.val_ratio, 
        args.test_ratio,
        args.output_dir
    )
    
    # åˆ†æåˆ†å‰²è´¨é‡
    analyze_split_quality(train_file, val_file, test_file)
    
    print(f"\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
    print(f"è®­ç»ƒ: python train_with_val.py --train-data {train_file} --val-data {val_file}")
    print(f"æµ‹è¯•: python test.py --test-data {test_file} --model ./output/model_final.pth")