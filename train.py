import os
import argparse
import torch
import warnings

warnings.filterwarnings("ignore", category=FutureWarning, message=".*torch.cuda.amp.autocast.*")

from detectron2.config import get_cfg
from detectron2 import model_zoo
from detectron2.data.datasets import register_coco_instances
from detectron2.data import MetadataCatalog, DatasetCatalog
from detectron2.engine import DefaultTrainer
from detectron2.evaluation import COCOEvaluator, inference_on_dataset
from detectron2.data import build_detection_test_loader

class ValidationTrainer(DefaultTrainer):
    """æ”¯æŒéªŒè¯çš„è®­ç»ƒå™¨"""
    
    @classmethod
    def build_evaluator(cls, cfg, dataset_name, output_folder=None):
        if output_folder is None:
            output_folder = os.path.join(cfg.OUTPUT_DIR, "inference")
        return COCOEvaluator(dataset_name, cfg, True, output_folder)

def register_datasets(train_json, val_json, data_dir):
    """æ³¨å†Œè®­ç»ƒé›†å’ŒéªŒè¯é›†"""
    
    # æ¸…é™¤å·²æœ‰æ³¨å†Œ
    for name in ["square_train", "square_val"]:
        if name in DatasetCatalog:
            DatasetCatalog.remove(name)
            MetadataCatalog.remove(name)
    
    # æ³¨å†Œè®­ç»ƒé›†
    register_coco_instances("square_train", {}, train_json, data_dir)
    
    # æ³¨å†ŒéªŒè¯é›†
    register_coco_instances("square_val", {}, val_json, data_dir)
    
    print("æ•°æ®é›†æ³¨å†ŒæˆåŠŸ:")
    print(f"  è®­ç»ƒé›†: {train_json}")
    print(f"  éªŒè¯é›†: {val_json}")
    
    return "square_train", "square_val"

def setup_config_with_validation(train_dataset, val_dataset, output_dir="./output", confidence_threshold=0.7):
    """é…ç½®æ”¯æŒéªŒè¯çš„è®­ç»ƒå‚æ•°"""
    cfg = get_cfg()
    cfg.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml"))
    
    # æ•°æ®é›†é…ç½®
    cfg.DATASETS.TRAIN = (train_dataset,)
    cfg.DATASETS.TEST = (val_dataset,)  # éªŒè¯é›†
    
    # è®­ç»ƒå‚æ•°
    cfg.SOLVER.IMS_PER_BATCH = 10
    cfg.DATALOADER.NUM_WORKERS = 6
    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml")
    
    cfg.SOLVER.BASE_LR = 0.01
    cfg.SOLVER.MAX_ITER = 1500
    cfg.SOLVER.STEPS = (800, 1200)
    cfg.SOLVER.GAMMA = 0.1
    cfg.SOLVER.WARMUP_ITERS = 200
    cfg.SOLVER.WARMUP_FACTOR = 1.0 / 1000
    cfg.SOLVER.WEIGHT_DECAY = 0.0001
    
    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128
    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1
    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = confidence_threshold
    cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.5
    
    cfg.MODEL.RPN.BATCH_SIZE_PER_IMAGE = 512
    
    cfg.INPUT.MIN_SIZE_TRAIN = (640, 672, 704, 736, 768, 800)
    cfg.INPUT.MAX_SIZE_TRAIN = 1333
    cfg.INPUT.MIN_SIZE_TEST = 800
    cfg.INPUT.MAX_SIZE_TEST = 1333
    
    cfg.TEST.DETECTIONS_PER_IMAGE = 100
    
    # è®¾å¤‡é…ç½®
    if torch.cuda.is_available():
        cfg.MODEL.DEVICE = "cuda"
        print("ä½¿ç”¨GPUè®­ç»ƒ")
    else:
        cfg.MODEL.DEVICE = "cpu"
        cfg.SOLVER.IMS_PER_BATCH = 4
        cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128
        cfg.MODEL.RPN.BATCH_SIZE_PER_IMAGE = 256
        print("ä½¿ç”¨CPUè®­ç»ƒ")
    
    cfg.OUTPUT_DIR = output_dir
    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
    
    # ğŸ” éªŒè¯é…ç½® - å…³é”®è®¾ç½®
    cfg.TEST.EVAL_PERIOD = 300  # æ¯300æ­¥éªŒè¯ä¸€æ¬¡
    cfg.SOLVER.CHECKPOINT_PERIOD = 500  # æ¯500æ­¥ä¿å­˜ä¸€æ¬¡
    cfg.SOLVER.AMP.ENABLED = True
    
    return cfg

def train_with_validation(train_json, val_json, data_dir, output_dir="./output", confidence_threshold=0.7):
    """æ”¯æŒéªŒè¯çš„è®­ç»ƒæµç¨‹"""
    print("å¼€å§‹è®­ç»ƒ (åŒ…å«éªŒè¯)")
    
    # æ³¨å†Œæ•°æ®é›†
    train_dataset, val_dataset = register_datasets(train_json, val_json, data_dir)
    
    # é…ç½®æ¨¡å‹
    cfg = setup_config_with_validation(train_dataset, val_dataset, output_dir, confidence_threshold)
    
    print(f"è®­ç»ƒé…ç½®:")
    print(f"  æ‰¹å¤§å°: {cfg.SOLVER.IMS_PER_BATCH}")
    print(f"  æœ€å¤§è¿­ä»£: {cfg.SOLVER.MAX_ITER}")
    print(f"  éªŒè¯é¢‘ç‡: æ¯{cfg.TEST.EVAL_PERIOD}æ­¥")
    print(f"  ç½®ä¿¡åº¦é˜ˆå€¼: {confidence_threshold}")
    
    # å¼€å§‹è®­ç»ƒ
    trainer = ValidationTrainer(cfg)
    trainer.resume_or_load(resume=False)
    trainer.train()
    
    print("è®­ç»ƒå®Œæˆ")
    
    # æœ€ç»ˆéªŒè¯
    print("è¿›è¡Œæœ€ç»ˆéªŒè¯...")
    cfg.MODEL.WEIGHTS = os.path.join(output_dir, "model_final.pth")
    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = confidence_threshold
    
    evaluator = COCOEvaluator(val_dataset, cfg, False, output_dir=output_dir)
    val_loader = build_detection_test_loader(cfg, val_dataset)
    trainer.model.eval()
    results = inference_on_dataset(trainer.model, val_loader, evaluator)
    
    print(f"\nğŸ“Š æœ€ç»ˆéªŒè¯ç»“æœ:")
    if 'segm' in results:
        print(f"åˆ†å‰²AP: {results['segm']['AP']:.3f}")
        print(f"åˆ†å‰²AP50: {results['segm']['AP50']:.3f}")
        print(f"åˆ†å‰²AP75: {results['segm']['AP75']:.3f}")
        print(f"åˆ†å‰²APs: {results['segm']['APs']:.3f}")
        print(f"åˆ†å‰²APm: {results['segm']['APm']:.3f}")
        print(f"åˆ†å‰²APl: {results['segm']['APl']:.3f}")
    
    if 'bbox' in results:
        print(f"æ£€æµ‹AP: {results['bbox']['AP']:.3f}")
        print(f"æ£€æµ‹AP50: {results['bbox']['AP50']:.3f}")
        print(f"æ£€æµ‹AP75: {results['bbox']['AP75']:.3f}")
    
    print(f"\næ¨¡å‹ä¿å­˜åœ¨: {os.path.join(output_dir, 'model_final.pth')}")
    
    return cfg, results

def quick_validation_test(model_path, val_json, data_dir, confidence_threshold=0.7):
    """å¿«é€ŸéªŒè¯æµ‹è¯•"""
    from detectron2.engine import DefaultPredictor
    from detectron2.utils.visualizer import Visualizer
    import cv2
    import matplotlib.pyplot as plt
    import json
    import random
    
    print(f"å¿«é€ŸéªŒè¯æµ‹è¯• (ç½®ä¿¡åº¦: {confidence_threshold})")
    
    # æ³¨å†ŒéªŒè¯é›†
    if "square_val_test" in DatasetCatalog:
        DatasetCatalog.remove("square_val_test")
        MetadataCatalog.remove("square_val_test")
    
    register_coco_instances("square_val_test", {}, val_json, data_dir)
    
    # é…ç½®é¢„æµ‹å™¨
    cfg = get_cfg()
    cfg.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml"))
    cfg.MODEL.WEIGHTS = model_path
    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1
    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = confidence_threshold
    cfg.MODEL.DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    
    predictor = DefaultPredictor(cfg)
    metadata = MetadataCatalog.get("square_val_test")
    if not hasattr(metadata, 'thing_classes'):
        metadata.thing_classes = ["square"]
    
    # æµ‹è¯•éªŒè¯é›†ä¸­çš„å›¾ç‰‡
    with open(val_json, 'r') as f:
        val_data = json.load(f)
    
    test_images = random.sample(val_data['images'], min(5, len(val_data['images'])))
    
    print(f"æµ‹è¯• {len(test_images)} å¼ éªŒè¯é›†å›¾ç‰‡:")
    
    for i, img_info in enumerate(test_images):
        img_path = os.path.join(data_dir, img_info['file_name'].replace("./", ""))
        
        if os.path.exists(img_path):
            im = cv2.imread(img_path)
            outputs = predictor(im)
            
            instances = outputs["instances"]
            scores = instances.scores.cpu().numpy() if len(instances) > 0 else []
            
            print(f"  {i+1}. {os.path.basename(img_path)}")
            print(f"     æ£€æµ‹æ•°é‡: {len(instances)}")
            if len(scores) > 0:
                print(f"     ç½®ä¿¡åº¦: {[f'{s:.3f}' for s in scores]}")
            
            # å¯è§†åŒ–ç¬¬ä¸€å¼ å›¾ç‰‡
            if i == 0:
                v = Visualizer(im[:, :, ::-1], metadata=metadata, scale=1.0)
                out = v.draw_instance_predictions(instances.to("cpu"))
                
                plt.figure(figsize=(12, 8))
                plt.imshow(out.get_image())
                plt.title(f"éªŒè¯é›†æµ‹è¯•æ ·ä¾‹ (æ£€æµ‹åˆ° {len(instances)} ä¸ªç›®æ ‡)")
                plt.axis('off')
                plt.savefig(os.path.join(os.path.dirname(model_path), "validation_test.png"))
                plt.show()

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='æ”¯æŒéªŒè¯é›†çš„è®­ç»ƒ')
    parser.add_argument('--train-data', type=str, required=True, help='è®­ç»ƒé›†JSONæ–‡ä»¶')
    parser.add_argument('--val-data', type=str, required=True, help='éªŒè¯é›†JSONæ–‡ä»¶')
    parser.add_argument('--data-dir', type=str, default='./', help='æ•°æ®æ ¹ç›®å½•')
    parser.add_argument('--output-dir', type=str, default='./output', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--confidence', type=float, default=0.7, help='ç½®ä¿¡åº¦é˜ˆå€¼')
    parser.add_argument('--test-only', action='store_true', help='ä»…æµ‹è¯•éªŒè¯é›†')
    parser.add_argument('--model-path', type=str, help='æµ‹è¯•æ¨¡å‹è·¯å¾„')
    
    args = parser.parse_args()
    
    if args.test_only and args.model_path:
        quick_validation_test(args.model_path, args.val_data, args.data_dir, args.confidence)
    else:
        cfg, results = train_with_validation(
            args.train_data, 
            args.val_data, 
            args.data_dir, 
            args.output_dir, 
            args.confidence
        )