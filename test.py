import os
import argparse
import torch
import warnings
import json
import cv2
import matplotlib.pyplot as plt
import numpy as np

warnings.filterwarnings("ignore", category=FutureWarning)

from detectron2.config import get_cfg
from detectron2 import model_zoo
from detectron2.data.datasets import register_coco_instances
from detectron2.data import MetadataCatalog, DatasetCatalog
from detectron2.engine import DefaultPredictor
from detectron2.utils.visualizer import Visualizer

def calculate_iou(pred_mask, gt_mask):
    """è®¡ç®—IoU"""
    intersection = np.logical_and(pred_mask, gt_mask).sum()
    union = np.logical_or(pred_mask, gt_mask).sum()
    if union == 0:
        return 0.0
    return intersection / union

def evaluate_predictions(predictor, test_images, data_dir, annotations, confidence, output_dir):
    """è¯„ä¼°é¢„æµ‹ç»“æœå¹¶ä¿å­˜å¯è§†åŒ–"""
    total_gt = 0
    total_pred = 0
    true_positives = 0
    false_positives = 0
    false_negatives = 0
    
    iou_scores = []
    detailed_results = []
    
    # åˆ›å»ºå¯è§†åŒ–è¾“å‡ºç›®å½•
    vis_dir = os.path.join(output_dir, "visualizations")
    os.makedirs(vis_dir, exist_ok=True)
    
    print(f"ğŸ“Š å¼€å§‹è¯„ä¼° {len(test_images)} å¼ å›¾ç‰‡...")
    metadata = MetadataCatalog.get("test_dataset")
    
    for idx, img_info in enumerate(test_images):
        img_path = os.path.join(data_dir, img_info['file_name'].replace("./", ""))
        
        if not os.path.exists(img_path):
            print(f"è·³è¿‡ä¸å­˜åœ¨çš„å›¾ç‰‡: {img_path}")
            continue
            
        # è·å–ground truth
        img_id = img_info['id']
        gt_annotations = [ann for ann in annotations if ann['image_id'] == img_id]
        
        # é¢„æµ‹
        image = cv2.imread(img_path)
        outputs = predictor(image)
        pred_instances = outputs["instances"].to("cpu")
        
        num_gt = len(gt_annotations)
        num_pred = len(pred_instances)
        
        total_gt += num_gt
        total_pred += num_pred
        
        # è®¡ç®—IoUå’ŒåŒ¹é…
        matched_pred = set()
        matched_gt = set()
        img_ious = []
        
        if num_pred > 0 and num_gt > 0:
            pred_masks = pred_instances.pred_masks.numpy()
            
            for gt_idx, gt_ann in enumerate(gt_annotations):
                best_iou = 0.0
                best_pred_idx = -1
                
                # åˆ›å»ºGT mask
                if 'segmentation' in gt_ann and gt_ann['segmentation']:
                    try:
                        from pycocotools import mask as maskUtils
                        h, w = img_info['height'], img_info['width']
                        rle = maskUtils.frPyObjects(gt_ann['segmentation'], h, w)
                        gt_mask = maskUtils.decode(rle).squeeze()
                        if gt_mask.ndim > 2:
                            gt_mask = gt_mask.any(axis=2)
                    except:
                        continue
                else:
                    continue
                
                # æ‰¾æœ€ä½³åŒ¹é…
                for pred_idx, pred_mask in enumerate(pred_masks):
                    if pred_idx in matched_pred:
                        continue
                    iou = calculate_iou(pred_mask, gt_mask)
                    if iou > best_iou:
                        best_iou = iou
                        best_pred_idx = pred_idx
                
                # å¦‚æœIoU > 0.5è®¤ä¸ºåŒ¹é…
                if best_iou > 0.5:
                    matched_pred.add(best_pred_idx)
                    matched_gt.add(gt_idx)
                    img_ious.append(best_iou)
        
        # ç»Ÿè®¡
        img_tp = len(matched_gt)
        img_fp = num_pred - img_tp
        img_fn = num_gt - img_tp
        
        true_positives += img_tp
        false_positives += img_fp
        false_negatives += img_fn
        
        # è®¡ç®—å•å¼ å›¾ç‰‡æŒ‡æ ‡
        img_precision = img_tp / max(num_pred, 1) if num_pred > 0 else 0.0
        img_recall = img_tp / max(num_gt, 1) if num_gt > 0 else 0.0
        img_avg_iou = np.mean(img_ious) if img_ious else 0.0
        
        iou_scores.extend(img_ious)
        
        detailed_results.append({
            'image': os.path.basename(img_path),
            'gt_count': num_gt,
            'pred_count': num_pred,
            'tp': img_tp,
            'fp': img_fp,
            'fn': img_fn,
            'precision': img_precision,
            'recall': img_recall,
            'avg_iou': img_avg_iou
        })
        
        # å¯è§†åŒ–å¹¶ä¿å­˜
        v = Visualizer(image[:, :, ::-1], metadata=metadata, scale=1.0)
        vis_result = v.draw_instance_predictions(pred_instances)
        
        # ä¿å­˜å¯è§†åŒ–å›¾ç‰‡
        output_path = os.path.join(vis_dir, f"{idx+1:03d}_{os.path.basename(img_path)}")
        
        plt.figure(figsize=(12, 8))
        plt.imshow(vis_result.get_image())
        
        title = (f'{os.path.basename(img_path)}\n'
                f'GT: {num_gt}, Pre: {num_pred}, '
                f'Correct: {img_tp}, Wrong: {img_fp}, Miss: {img_fn}\n'
                f'accurancy: {img_precision:.3f}, recall: {img_recall:.3f}, IoU: {img_avg_iou:.3f}')
        
        plt.title(title, fontsize=10)
        plt.axis('off')
        plt.savefig(output_path, bbox_inches='tight', dpi=150)
        plt.close()  # å…³é—­å›¾çª—ä»¥èŠ‚çœå†…å­˜
        
        # æ‰“å°è¿›åº¦
        if (idx + 1) % 10 == 0:
            print(f"  å·²å¤„ç†: {idx+1}/{len(test_images)}")
    
    # è®¡ç®—æ€»ä½“æŒ‡æ ‡
    overall_precision = true_positives / max(total_pred, 1) if total_pred > 0 else 0.0
    overall_recall = true_positives / max(total_gt, 1) if total_gt > 0 else 0.0
    overall_f1 = 2 * (overall_precision * overall_recall) / max(overall_precision + overall_recall, 1e-6)
    overall_iou = np.mean(iou_scores) if iou_scores else 0.0
    
    # è¾“å‡ºç»“æœ
    print(f"\nğŸ¯ æ€»ä½“è¯„ä¼°ç»“æœ:")
    print(f"  æµ‹è¯•å›¾ç‰‡æ•°: {len(test_images)}")
    print(f"  æ€»GTç›®æ ‡æ•°: {total_gt}")
    print(f"  æ€»é¢„æµ‹æ•°: {total_pred}")
    print(f"  æ­£ç¡®æ£€æµ‹(TP): {true_positives}")
    print(f"  è¯¯æ£€æµ‹(FP): {false_positives}")  
    print(f"  æ¼æ£€æµ‹(FN): {false_negatives}")
    print(f"")
    print(f"ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
    print(f"  å‡†ç¡®ç‡(Precision): {overall_precision:.3f} ({true_positives}/{total_pred})")
    print(f"  å¬å›ç‡(Recall): {overall_recall:.3f} ({true_positives}/{total_gt})")
    print(f"  F1åˆ†æ•°: {overall_f1:.3f}")
    print(f"  å¹³å‡IoU: {overall_iou:.3f}")
    print(f"  æ£€æµ‹ç‡: {true_positives/max(total_gt,1)*100:.1f}%")
    print(f"  è¯¯æ£€ç‡: {false_positives/max(total_pred,1)*100:.1f}%")
    print(f"")
    print(f"ğŸ“ å¯è§†åŒ–ç»“æœä¿å­˜åœ¨: {vis_dir}")
    
    return detailed_results, {
        'precision': overall_precision,
        'recall': overall_recall,
        'f1': overall_f1,
        'avg_iou': overall_iou,
        'tp': true_positives,
        'fp': false_positives,
        'fn': false_negatives
    }

def test_model(model_path, data_file, data_dir, confidence=0.7, output_dir="./test_results"):
    """æµ‹è¯•æ¨¡å‹å¹¶ä¿å­˜ç»“æœ"""
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # æ³¨å†Œæ•°æ®é›†
    if "test_dataset" in DatasetCatalog:
        DatasetCatalog.remove("test_dataset")
        MetadataCatalog.remove("test_dataset")
    
    register_coco_instances("test_dataset", {}, data_file, data_dir)
    metadata = MetadataCatalog.get("test_dataset")
    metadata.thing_classes = ["square"]
    
    # åŠ è½½æ¨¡å‹
    cfg = get_cfg()
    cfg.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml"))
    cfg.MODEL.WEIGHTS = model_path
    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1
    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = confidence
    cfg.MODEL.DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    
    predictor = DefaultPredictor(cfg)
    
    # è¯»å–æµ‹è¯•æ•°æ®
    with open(data_file, 'r') as f:
        data = json.load(f)
    
    print(f"æµ‹è¯•æ‰€æœ‰ {len(data['images'])} å¼ å›¾ç‰‡ (ç½®ä¿¡åº¦: {confidence})")
    
    # è¯„ä¼°æ‰€æœ‰å›¾ç‰‡
    detailed_results, overall_metrics = evaluate_predictions(
        predictor, data['images'], data_dir, data['annotations'], confidence, output_dir
    )
    
    # ä¿å­˜è¯¦ç»†ç»“æœåˆ°CSV
    import csv
    csv_path = os.path.join(output_dir, "detailed_results.csv")
    with open(csv_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=['image', 'gt_count', 'pred_count', 'tp', 'fp', 'fn', 'precision', 'recall', 'avg_iou'])
        writer.writeheader()
        writer.writerows(detailed_results)
    
    # ä¿å­˜æ€»ä½“æŒ‡æ ‡
    summary_path = os.path.join(output_dir, "evaluation_summary.txt")
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write(f"æ¨¡å‹è¯„ä¼°æ€»ç»“\n")
        f.write(f"æ¨¡å‹è·¯å¾„: {model_path}\n")
        f.write(f"æ•°æ®æ–‡ä»¶: {data_file}\n")
        f.write(f"ç½®ä¿¡åº¦é˜ˆå€¼: {confidence}\n\n")
        f.write(f"æ€»ä½“æŒ‡æ ‡:\n")
        f.write(f"  å‡†ç¡®ç‡: {overall_metrics['precision']:.3f}\n")
        f.write(f"  å¬å›ç‡: {overall_metrics['recall']:.3f}\n")
        f.write(f"  F1åˆ†æ•°: {overall_metrics['f1']:.3f}\n")
        f.write(f"  å¹³å‡IoU: {overall_metrics['avg_iou']:.3f}\n")
        f.write(f"  TP: {overall_metrics['tp']}\n")
        f.write(f"  FP: {overall_metrics['fp']}\n")
        f.write(f"  FN: {overall_metrics['fn']}\n")
    
    print(f"ğŸ“„ è¯¦ç»†ç»“æœCSV: {csv_path}")
    print(f"ğŸ“‹ è¯„ä¼°æ‘˜è¦: {summary_path}")
    
    return overall_metrics

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--model', type=str, required=True, help='æ¨¡å‹è·¯å¾„')
    parser.add_argument('--data', type=str, required=True, help='æ•°æ®æ–‡ä»¶ (val.jsonæˆ–test.json)')
    parser.add_argument('--data-dir', type=str, default='./', help='æ•°æ®ç›®å½•')
    parser.add_argument('--confidence', type=float, default=0.7, help='ç½®ä¿¡åº¦')
    parser.add_argument('--output', type=str, default='./test_results', help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    metrics = test_model(args.model, args.data, args.data_dir, args.confidence, args.output)
    
    if metrics:
        print(f"\nğŸ† Final Evaluation Summary:")
        print(f"  Precision: {metrics['precision']:.3f}")
        print(f"  Recall: {metrics['recall']:.3f}")  
        print(f"  F1-Score: {metrics['f1']:.3f}")
        print(f"  Average IoU: {metrics['avg_iou']:.3f}")
    else:
        print("âŒ Evaluation failed")